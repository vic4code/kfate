{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Upload Data Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipeline` is distributed along with [fate_client](https://pypi.org/project/fate-client/).\n",
    "\n",
    "```bash\n",
    "pip install fate_client\n",
    "```\n",
    "\n",
    "To use Pipeline, we need to first specify which `FATE Flow Service` to connect to. Once `fate_client` installed, one can find an cmd enterpoint name `pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: pipeline [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "Options:\r\n",
      "  --help  Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  config  pipeline config tool\r\n",
      "  init    \b - DESCRIPTION: Pipeline Config Command.\r\n"
     ]
    }
   ],
   "source": [
    "!pipeline --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a `FATE Flow Service` in 127.0.0.1:9380(defaults in standalone), then exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configuration succeeded.\r\n"
     ]
    }
   ],
   "source": [
    "!pipeline init --ip fateflow --port 9380"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before start a modeling task, the data to be used should be uploaded. \n",
    " Typically, a party is usually a cluster which include multiple nodes. Thus, when we upload these data, the data will be allocated to those nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.backend.pipeline import PipeLine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `pipeline` instance:\n",
    "\n",
    "    - initiator: \n",
    "        * role: guest\n",
    "        * party: 9999\n",
    "    - roles:\n",
    "        * guest: 9999\n",
    "\n",
    "note that only local party id is needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_upload = PipeLine().set_initiator(role='guest', party_id=9999).set_roles(guest=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define partitions for data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define table name and namespace, which will be used in FATE job configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_data_guest = {\"name\": \"breast_hetero_guest\", \"namespace\": f\"experiment\"}\n",
    "dense_data_host = {\"name\": \"breast_hetero_host\", \"namespace\": f\"experiment\"}\n",
    "tag_data = {\"name\": \"breast_hetero_host\", \"namespace\": f\"experiment\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/projects/fate/persistence/fate/doc/tutorial/pipeline'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add data to be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_base = \"/data/projects/fate/persistence/fate/\"\n",
    "pipeline_upload.add_upload_data(file=os.path.join(data_base, \"examples/data/breast_hetero_guest.csv\"),\n",
    "                                table_name=dense_data_guest[\"name\"],             # table name\n",
    "                                namespace=dense_data_guest[\"namespace\"],         # namespace\n",
    "                                head=1, partition=partition)               # data info\n",
    "\n",
    "pipeline_upload.add_upload_data(file=os.path.join(data_base, \"examples/data/breast_hetero_host.csv\"),\n",
    "                                table_name=dense_data_host[\"name\"],\n",
    "                                namespace=dense_data_host[\"namespace\"],\n",
    "                                head=1, partition=partition)\n",
    "\n",
    "pipeline_upload.add_upload_data(file=os.path.join(data_base, \"examples/data/breast_hetero_host.csv\"),\n",
    "                                table_name=tag_data[\"name\"],\n",
    "                                namespace=tag_data[\"namespace\"],\n",
    "                                head=1, partition=partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-20 09:06:03.805\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[31m\u001b[1mAn error has been caught in function '<module>', process 'MainProcess' (2038), thread 'MainThread' (140089512507200):\u001b[0m\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"/usr/local/lib/python3.8/site-packages/pipeline/utils/invoker/job_submitter.py\", line 61, in upload_data\n",
      "    raise ValueError\n",
      "\n",
      "\u001b[31m\u001b[1mValueError\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m\u001b[1mTraceback (most recent call last):\u001b[0m\n",
      "\n",
      "  File \"/usr/local/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "           │         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "           │         └ <code object <module> at 0x7f69217b4df0, file \"/usr/local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 1>\n",
      "           └ <function _run_code at 0x7f69217c5940>\n",
      "  File \"/usr/local/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "         │     └ {'__name__': '__main__', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nThis is separate from the ipykernel pack...\n",
      "         └ <code object <module> at 0x7f69217b4df0, file \"/usr/local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 1>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "    │   └ <bound method Application.launch_instance of <class 'ipykernel.kernelapp.IPKernelApp'>>\n",
      "    └ <module 'ipykernel.kernelapp' from '/usr/local/lib/python3.8/site-packages/ipykernel/kernelapp.py'>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n",
      "    app.start()\n",
      "    │   └ <function IPKernelApp.start at 0x7f691d3651f0>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7f69218d3700>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 711, in start\n",
      "    self.io_loop.start()\n",
      "    │    │       └ <function BaseAsyncIOLoop.start at 0x7f691f0888b0>\n",
      "    │    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f691d364550>\n",
      "    └ <ipykernel.kernelapp.IPKernelApp object at 0x7f69218d3700>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "    │    │            └ <function BaseEventLoop.run_forever at 0x7f6920fb9b80>\n",
      "    │    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "    └ <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f691d364550>\n",
      "  File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "    │    └ <function BaseEventLoop._run_once at 0x7f6920fbb700>\n",
      "    └ <_UnixSelectorEventLoop running=True closed=False debug=False>\n",
      "  File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "    │      └ <function Handle._run at 0x7f6920ffe4c0>\n",
      "    └ <Handle <TaskWakeupMethWrapper object at 0x7f6856ffc970>(<Future finis...f60>, ...],))>)>\n",
      "  File \"/usr/local/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    │    │            │    │           │    └ <member '_args' of 'Handle' objects>\n",
      "    │    │            │    │           └ <Handle <TaskWakeupMethWrapper object at 0x7f6856ffc970>(<Future finis...f60>, ...],))>)>\n",
      "    │    │            │    └ <member '_callback' of 'Handle' objects>\n",
      "    │    │            └ <Handle <TaskWakeupMethWrapper object at 0x7f6856ffc970>(<Future finis...f60>, ...],))>)>\n",
      "    │    └ <member '_context' of 'Handle' objects>\n",
      "    └ <Handle <TaskWakeupMethWrapper object at 0x7f6856ffc970>(<Future finis...f60>, ...],))>)>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "          │    └ <function Kernel.process_one at 0x7f691dcc7160>\n",
      "          └ <ipykernel.ipkernel.IPythonKernel object at 0x7f691d32a700>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "          │         └ ([<zmq.sugar.frame.Frame object at 0x7f691c2e8b40>, <zmq.sugar.frame.Frame object at 0x7f691c2e8bf0>, <zmq.sugar.frame.Frame ...\n",
      "          └ <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object at 0x7f691d32a700>>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "          └ <coroutine object Kernel.execute_request at 0x7f691d34dc40>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "                          └ <coroutine object IPythonKernel.do_execute at 0x7f68569a4a40>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n",
      "    res = shell.run_cell(\n",
      "          │     └ <function ZMQInteractiveShell.run_cell at 0x7f691d354940>\n",
      "          └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "                             │       └ {'store_history': True, 'silent': False, 'cell_id': None}\n",
      "                             └ ('pipeline_upload.upload(drop=1)',)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n",
      "    result = self._run_cell(\n",
      "             │    └ <function InteractiveShell._run_cell at 0x7f691e755700>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n",
      "    return runner(coro)\n",
      "           │      └ <coroutine object InteractiveShell.run_cell_async at 0x7f68569a49c0>\n",
      "           └ <function _pseudo_sync_runner at 0x7f691e748040>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "    │    └ <method 'send' of 'coroutine' objects>\n",
      "    └ <coroutine object InteractiveShell.run_cell_async at 0x7f68569a49c0>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "                       │    │             │        │     └ '/tmp/ipykernel_2038/330354976.py'\n",
      "                       │    │             │        └ [<_ast.Expr object at 0x7f68569db790>]\n",
      "                       │    │             └ <_ast.Module object at 0x7f68569dba30>\n",
      "                       │    └ <function InteractiveShell.run_ast_nodes at 0x7f691e7559d0>\n",
      "                       └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "             │    │        │     │              └ False\n",
      "             │    │        │     └ <ExecutionResult object at 7f6856b8cdf0, execution_count=25 error_before_exec=None error_in_exec=None info=<ExecutionInfo obj...\n",
      "             │    │        └ <code object <module> at 0x7f6856ee3f50, file \"/tmp/ipykernel_2038/330354976.py\", line 1>\n",
      "             │    └ <function InteractiveShell.run_code at 0x7f691e755a60>\n",
      "             └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "         │         │    │               │    └ {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, ...\n",
      "         │         │    │               └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "         │         │    └ <property object at 0x7f691e7459a0>\n",
      "         │         └ <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f691d32ac70>\n",
      "         └ <code object <module> at 0x7f6856ee3f50, file \"/tmp/ipykernel_2038/330354976.py\", line 1>\n",
      "\n",
      "> File \"\u001b[32m/tmp/ipykernel_2038/\u001b[0m\u001b[32m\u001b[1m330354976.py\u001b[0m\", line \u001b[33m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[1mpipeline_upload\u001b[0m\u001b[35m\u001b[1m.\u001b[0m\u001b[1mupload\u001b[0m\u001b[1m(\u001b[0m\u001b[1mdrop\u001b[0m\u001b[35m\u001b[1m=\u001b[0m\u001b[34m\u001b[1m1\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[36m│               └ \u001b[0m\u001b[36m\u001b[1m<function PipeLine.upload at 0x7f68571df5e0>\u001b[0m\n",
      "    \u001b[36m└ \u001b[0m\u001b[36m\u001b[1m<pipeline.backend.pipeline.PipeLine object at 0x7f6856e90cd0>\u001b[0m\n",
      "\n",
      "  File \"/usr/local/lib/python3.8/site-packages/pipeline/backend/pipeline.py\", line 664, in upload\n",
      "    self._train_job_id, detail_info = self._job_invoker.upload_data(upload_conf, int(drop))\n",
      "    │    │                            │    │            │           │                └ 1\n",
      "    │    │                            │    │            │           └ {'file': '/data/projects/fate/persistence/fate/examples/data/breast_hetero_guest.csv', 'table_name': 'breast_hetero_guest', '...\n",
      "    │    │                            │    │            └ <function JobInvoker.upload_data at 0x7f68571cfa60>\n",
      "    │    │                            │    └ <pipeline.utils.invoker.job_submitter.JobInvoker object at 0x7f6856c549a0>\n",
      "    │    │                            └ <pipeline.backend.pipeline.PipeLine object at 0x7f6856e90cd0>\n",
      "    │    └ None\n",
      "    └ <pipeline.backend.pipeline.PipeLine object at 0x7f6856e90cd0>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/pipeline/utils/invoker/job_submitter.py\", line 69, in upload_data\n",
      "    raise ValueError(\"job submit failed, err msg: {}\".format(result))\n",
      "                                                             └ {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}\n",
      "\n",
      "\u001b[31m\u001b[1mValueError\u001b[0m:\u001b[1m job submit failed, err msg: {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "job submit failed, err msg: {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pipeline/utils/invoker/job_submitter.py:61\u001b[0m, in \u001b[0;36mJobInvoker.upload_data\u001b[0;34m(self, submit_conf, drop)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretcode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretcode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjobId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[0;31mValueError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline_upload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/loguru/_logger.py:1226\u001b[0m, in \u001b[0;36mLogger.catch.<locals>.Catcher.__call__.<locals>.catch_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcatch_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m catcher:\n\u001b[0;32m-> 1226\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pipeline/backend/pipeline.py:664\u001b[0m, in \u001b[0;36mPipeLine.upload\u001b[0;34m(self, drop)\u001b[0m\n\u001b[1;32m    662\u001b[0m upload_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_upload_conf(data_conf)\n\u001b[1;32m    663\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload_conf is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(upload_conf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_job_id, detail_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job_invoker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupload_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_board_url \u001b[38;5;241m=\u001b[39m detail_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboard_url\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job_invoker\u001b[38;5;241m.\u001b[39mmonitor_job_status(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_job_id,\n\u001b[1;32m    667\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    668\u001b[0m                                      \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/pipeline/utils/invoker/job_submitter.py:69\u001b[0m, in \u001b[0;36mJobInvoker.upload_data\u001b[0;34m(self, submit_conf, drop)\u001b[0m\n\u001b[1;32m     67\u001b[0m     data \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob submit failed, err msg: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(result))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m job_id, data\n",
      "\u001b[0;31mValueError\u001b[0m: job submit failed, err msg: {'retcode': 100, 'retmsg': 'Connection refused, Please check if the fate flow service is started'}"
     ]
    }
   ],
   "source": [
    "pipeline_upload.upload(drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more demo on using pipeline to submit jobs, please refer to [pipeline demos](https://github.com/FederatedAI/FATE/tree/master/examples/pipeline/demo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
